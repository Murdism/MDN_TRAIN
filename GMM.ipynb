{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt # creating visualizations\n",
    "import numpy as np # basic math and random numbers\n",
    "import torch # package for building functions with learnable parameters\n",
    "import torch.nn as nn # prebuilt functions specific to neural networks\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "#from disp import ADE,FDE,prediction_displacement,ADE_double_coordinates,FDE_double_coordinates,prediction_displacement_double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADE(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    pred = pd.DataFrame(pred.cpu().numpy())\n",
    "    truth = pd.DataFrame(truth.cpu().numpy())\n",
    "    print(pred.shape)\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        for j in range (half):\n",
    "\n",
    "            a = np.array((pred[i][j] , pred[i][half+j]))\n",
    "            b = np.array((truth.iloc[i][j] , truth.iloc[i][half+j]))\n",
    "\n",
    "            dist = np.linalg.norm(a-b)\n",
    "            sum+=dist\n",
    "            counter+=1\n",
    "            #print(\"Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "\n",
    "    return (sum/counter)\n",
    "def FDE_Single(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    pred = pred.cpu().numpy()\n",
    "    truth = truth.cpu().numpy()\n",
    "\n",
    "    for ai,bi in zip(pred,truth):\n",
    "        # print(ai,bi)\n",
    "        dist = np.linalg.norm(ai-bi)\n",
    "        sum+=dist\n",
    "        counter+=1\n",
    "    return (sum/counter)\n",
    "\n",
    "\n",
    "def show_error_single(prediction_train,prediction_test,train_target,test_target):\n",
    "                # # Show error rate\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Train: \",prediction_displacement(train_target))\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Test: \",prediction_displacement(test_target))\n",
    "        print(\"////////////////////////////////////////////////\")\n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TEST: \", FDE_Single(prediction_test,test_target))\n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TRAIN: \", FDE_Single(prediction_train,train_target))\n",
    "        print(\"//////////////////////////////////////////\")\n",
    "        #Final_displacement_error\n",
    "        print(\"FDE ERROR RATE TEST: \", FDE_Single(prediction_test,test_target))\n",
    "        print(\"FDE ERROR RATE TRAIN: \", FDE_Single(prediction_train,train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Predict error\n",
    "# Average Displacement error\n",
    "\n",
    "def ADE(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        for j in range (half):\n",
    "\n",
    "            a = np.array((pred[i][j] , pred[i][half+j]))\n",
    "            b = np.array((truth.iloc[i][j] , truth.iloc[i][half+j]))\n",
    "\n",
    "            dist = np.linalg.norm(a-b)\n",
    "            sum+=dist\n",
    "            counter+=1\n",
    "            #print(\"Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "\n",
    "    return (sum/counter)\n",
    "def prediction_displacement_double(pred): \n",
    "          \n",
    "        sum=0\n",
    "        counter=0        \n",
    "        pred=np.array(pred)\n",
    "        last_index= (len(pred[0])-1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "                    a = np.array((pred[i][0][0] , pred[i][0][1]))\n",
    "                    b = np.array((pred[i][last_index][0] , pred[i][last_index][1]))\n",
    "\n",
    "                    dist = np.linalg.norm(a-b)\n",
    "                    sum+=dist\n",
    "                    counter+=1\n",
    "        return (sum/counter)\n",
    "\n",
    "def prediction_displacement(pred): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    pred=np.array(pred)\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        last=(len(pred[i]) - 1)\n",
    "\n",
    "        a = np.array((pred[i][0] , pred[i][half]))\n",
    "        b = np.array((pred[i][half-1] , pred[i][last]))\n",
    "\n",
    "        dist = np.linalg.norm(a-b)\n",
    "        sum+=dist\n",
    "        counter+=1\n",
    "        #print(\"FDE Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "            \n",
    "    return (sum/counter)\n",
    "def FDE(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        last=(len(pred[i]) - 1)\n",
    "\n",
    "        a = np.array((pred[i][half-1] , pred[i][last]))\n",
    "        b = np.array((truth.iloc[i][half-1] , truth.iloc[i][last]))\n",
    "\n",
    "        dist = np.linalg.norm(a-b)\n",
    "        sum+=dist\n",
    "        counter+=1\n",
    "        #print(\"FDE Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "            \n",
    "    return (sum/counter)\n",
    "\n",
    "\n",
    "def FDE_double_coordinates(pred,truth):\n",
    "          \n",
    "        sum=0\n",
    "        counter=0\n",
    "        \n",
    "        pred=np.array(pred)\n",
    "        truth=np.array(truth)\n",
    "        last_index= (len(pred[0])-1)\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "            a = np.array((pred[i][last_index][0] , pred[i][last_index][1]))\n",
    "            b = np.array((truth[i][last_index][0] , truth[i][last_index][1]))\n",
    "\n",
    "            dist = np.linalg.norm(a-b)\n",
    "            sum+=dist\n",
    "            counter+=1\n",
    "\n",
    "                # for j in range (len(pred[i])):\n",
    "                #    pred_x.append(pred[i][j][0])  \n",
    "                #    pred_y.append(pred[i][j][1]) \n",
    "\n",
    "                #    truth_x.append(truth[i][j][0])  \n",
    "                #    truth_y.append(truth[i][j][1])\n",
    "        return (sum/counter)\n",
    "def ADE_double_coordinates(pred,truth):\n",
    "          \n",
    "        sum=0\n",
    "        counter=0\n",
    "        \n",
    "        pred=np.array(pred)\n",
    "        truth=np.array(truth)\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "                for j in range (len(pred[i])):\n",
    "\n",
    "                    a = np.array((pred[i][j][0] , pred[i][j][1]))\n",
    "                    b = np.array((truth[i][j][0] , truth[i][j][1]))\n",
    "\n",
    "                    dist = np.linalg.norm(a-b)\n",
    "                    sum+=dist\n",
    "                    counter+=1\n",
    "        return (sum/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('train_input.pickle', 'rb') as data:\n",
    "     train_input = pickle.load(data)\n",
    "    with open('validate_input.pickle', 'rb') as data:\n",
    "         validate_input= pickle.load(data)\n",
    "    with open('test_input.pickle', 'rb') as data:\n",
    "         test_input = pickle.load(data)\n",
    "    with open('train_target.pickle', 'rb') as data:\n",
    "          train_target = pickle.load(data)\n",
    "    with open('validate_target.pickle', 'rb') as data:\n",
    "          validate_target = pickle.load(data)\n",
    "    with open('test_target.pickle', 'rb') as data:\n",
    "         test_target = pickle.load(data)\n",
    "    return torch.tensor(train_input,dtype=torch.float32), torch.tensor(validate_input,dtype=torch.float32), torch.tensor(test_input,dtype=torch.float32), torch.tensor(train_target,dtype=torch.float32), torch.tensor(validate_target,dtype=torch.float32), torch.tensor(test_target,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  torch.Size([5099, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "train_input,validate_input,test_input,train_target,validate_target,test_target=load_data()\n",
    "print(\"Train shape: \" ,train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, x, y):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "      #   self.start = 0\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x)\n",
    "      \n",
    "  def base(self,index):\n",
    "      return self.x[index][0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # Normalize\n",
    "        # Load data and get label\n",
    "        \n",
    "      #   self.start = self.x[index][0]\n",
    "        X = self.x[index].to(device)- self.x[index][0].expand_as(self.x[index]).to(device)\n",
    "      \n",
    "        Y = self.y[index].to(device) - self.x[index][0].expand_as(self.y[index]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # X, y,start ---> start is the first x and y of the sample\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= Dataset(train_input, train_target)\n",
    "test_dataset= Dataset(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000],\n",
       "         [-0.4405,  0.0222],\n",
       "         [-0.8810,  0.0444],\n",
       "         [-1.3215,  0.0666],\n",
       "         [-1.7626,  0.0866],\n",
       "         [-2.2088,  0.0866],\n",
       "         [-2.6550,  0.0866],\n",
       "         [-3.1012,  0.0866]], device='cuda:0'),\n",
       " tensor([[ 0.0000,  0.0000],\n",
       "         [-0.4405,  0.0222],\n",
       "         [-0.8810,  0.0444],\n",
       "         [-1.3221,  0.0644],\n",
       "         [-1.7683,  0.0644],\n",
       "         [-2.2145,  0.0644],\n",
       "         [-2.6607,  0.0644],\n",
       "         [-3.1069,  0.0644]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0],train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNetwork, self).__init__()\n",
    "        self.output_shape = (8,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.unflatten = nn.Unflatten(1,self.output_shape)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(8*2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 8*2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # print(x[0])\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        out = self.unflatten(logits)\n",
    "        # print(out[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(8, 2))\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LinearNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_input. requires_grad = True\n",
    "train_input = train_input.to(device)\n",
    "train_target = train_target.to(device)\n",
    "train_input. requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(3000):\n",
    "        y_pred = model(train_input) # make a prediction\n",
    "        loss = loss_fn(y_pred,train_target) # compute the loss\n",
    "        optimizer.zero_grad() # prepare the optimizer\n",
    "        loss.backward() # compute the contribution of each parameter to the loss\n",
    "        optimizer.step() # modify the parameters\n",
    "\n",
    "        if epoch % 300 == 0:\n",
    "            print(epoch, loss.data)\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_double(prediction_train,prediction_test,train_target,test_target):\n",
    "                # # Show error rate\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Train: \",prediction_displacement_double(train_target))\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Test: \",prediction_displacement_double(test_target))\n",
    "             \n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TEST: \", ADE_double_coordinates(prediction_test,test_target))\n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TRAIN: \", ADE_double_coordinates(prediction_train,train_target))\n",
    "        print(\"//////////////////////////////////////////\")\n",
    "        #Final_displacement_error\n",
    "        print(\"FDE ERROR RATE TEST: \", FDE_double_coordinates(prediction_test,test_target))\n",
    "        print(\"FDE ERROR RATE TRAIN: \", FDE_double_coordinates(prediction_train,train_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_units,out_features=16,isseq=True):\n",
    "        super().__init__()\n",
    "        self.seq = isseq\n",
    "        self.num_features = num_features  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # print(\"batch_size\",batch_size)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_().to(device)\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        if(self.seq):\n",
    "            out = self.linear(hn[0]).reshape(batch_size,8,2) # First dim of Hn is num_layers, which is set to 1 above.\n",
    "        else:\n",
    "            out = self.linear(hn[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_hidden_units = 16\n",
    "hidden_size = 12\n",
    "n_features = 2\n",
    "num_layers = 1\n",
    "batch_size = 10\n",
    "\n",
    "LSTM_model = ShallowRegressionLSTM(num_features=n_features, hidden_units=num_hidden_units).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer,epochs=10):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i in range (epochs):\n",
    "        total_loss = 0\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            # print(output.shape)\n",
    "            # print(y.shape)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f\"Epoch {i+1} train loss: {avg_loss}\")\n",
    "    print(f\"Epoch {i+1} train loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(train_loader, LSTM_model, loss_function, optimizer,epochs=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.0596, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_input = test_input.to(device)\n",
    "    test_pred = LSTM_model(test_input)\n",
    "    train_pred = LSTM_model(train_input)\n",
    "    loss = loss_fn(test_target.to(device), test_pred)\n",
    "    print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train_pred.cpu()\n",
    "test_pred = test_pred.cpu()\n",
    "train_target = train_target.cpu()\n",
    "test_target = test_target.cpu()\n",
    "#show_error_double(train_pred,test_pred,train_target,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting only one point in the future\n",
    "train_target_one = train_target[:,0,:]\n",
    "test_target_one = test_target[:,0,:]\n",
    "train_target_one = train_target_one.to(device)\n",
    "test_target_one = test_target_one.to(device)\n",
    "train_dataset_one= Dataset(train_input, train_target_one)\n",
    "test_dataset_one= Dataset(test_input, test_target_one)\n",
    "train_loader_one = torch.utils.data.DataLoader(train_dataset_one, batch_size)\n",
    "#test_loader_one = torch.utils.data.DataLoader(test_dataset_one, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_hidden_units = 16\n",
    "hidden_size = 12\n",
    "n_features = 2\n",
    "num_layers = 1\n",
    "batch_size = 5\n",
    "\n",
    "LSTM_model2 = ShallowRegressionLSTM(num_features=n_features, hidden_units=num_hidden_units,out_features=2,isseq=False).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(LSTM_model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(train_loader_one, LSTM_model2, loss_function, optimizer,epochs=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train: 43.23713684082031 \n",
      "loss_test: 42.55158233642578\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_pred_one= LSTM_model2(test_input)\n",
    "    train_pred_one = LSTM_model2(train_input)\n",
    "    loss_train = loss_fn(train_target_one.to(device), train_pred_one)\n",
    "    loss_test = loss_fn(test_target_one.to(device), test_pred_one)\n",
    "    print(f'loss_train: {loss_train.data} \\nloss_test: {loss_test.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_one = train_pred_one.cpu()\n",
    "test_pred_one = test_pred_one.cpu()\n",
    "train_target_one = train_target_one.cpu()\n",
    "test_target_one = test_target_one.cpu()\n",
    "#show_error_single(train_pred_one,test_pred_one,train_target_one,test_target_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Distribution Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol{\\Sigma}|}} \\exp\\left( -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\mathrm{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right)\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multivariate_gaussian_older(pos, mu, Sigma):\n",
    "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "\n",
    "    pos is an array constructed by packing the meshed arrays of variables\n",
    "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = mu.shape[0]\n",
    "    Sigma_det = np.linalg.det(Sigma)\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "    # way across all the input variables.\n",
    "    print(\"Old multivariate distribution\")\n",
    "    print(f\"mu.shape: {mu.shape}, Sigma: {Sigma.shape}\")\n",
    "    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
    "\n",
    "    return np.exp(-fac / 2) / N\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneDivSqrtTwoPI = 1.0 / np.sqrt(2.0*np.pi) # normalization factor for Gaussians\n",
    "def bivirate_distribution(sigma_x,sigma_y, mu_x , mu_y,input):\n",
    "    # make |mu|=K copies of y, subtract mu, divide by sigma\n",
    "    x=input[:,0]\n",
    "    x = x.unsqueeze(dim=1).to(device)\n",
    "    y=input[:,1]\n",
    "    y = y.unsqueeze(dim=1).to(device)\n",
    "    result_x = torch.square((x.expand_as(mu_x) - mu_x) * torch.reciprocal(sigma_x))\n",
    "    result_y = torch.square((y.expand_as(mu_y) - mu_y) * torch.reciprocal(sigma_y))\n",
    "    result = -0.5*(result_x + result_y)\n",
    "    sigma = sigma_x*sigma_y\n",
    "    return (torch.exp(result) * torch.reciprocal(sigma)) * oneDivSqrtTwoPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN_bivirate(nn.Module):\n",
    "    def __init__(self,n_hidden, n_gaussians,input_shape = 16):\n",
    "        super(MDN_bivirate, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(input_shape, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_sigma_x = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_sigma_y = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_mu_x = nn.Linear(n_hidden, n_gaussians).to(device)  \n",
    "        self.z_mu_y = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x.shape: \",x)\n",
    "        x = self.flatten(x)\n",
    "        z_h = self.z_h(x).to(device)\n",
    "        # print(\"z_h.shape: \",torch.isnan(z_h).any())\n",
    "        if(torch.isnan(z_h).any()):\n",
    "            print(x)\n",
    "            # for param in model.parameters():\n",
    "            #     print(\"---------------------\",param.data)\n",
    "        pi = nn.functional.softmax(self.z_pi(z_h), -1).to(device)\n",
    "        # print(\"pi.shape: \",pi)\n",
    "        sigma_x = torch.exp(self.z_sigma_x(z_h)).to(device)\n",
    "        # print(\"pi.shape: \",sigma_x)\n",
    "        sigma_y = torch.exp(self.z_sigma_y(z_h)).to(device)\n",
    "        mu_x = self.z_mu_x(z_h).to(device)\n",
    "        mu_y = self.z_mu_y(z_h)\n",
    "\n",
    "        #print(torch.isnan(pi))\n",
    "        # print(f\"pi : {pi}\")\n",
    "        # print(f\"sigma_x : {sigma_x}\")\n",
    "\n",
    "        # print(\"sigma.shape: \",sigma.shape)\n",
    "        return pi, sigma_x,sigma_y, mu_x ,mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(pi, sigma_x,sigma_y, mu_x , mu_y,y):\n",
    "    # calculate the score for each mixture of the gaussian_distribution\n",
    "    # input shape (sample_size,num_mixtures,parameter) parametr is 2 in mue (x,y) and 2,2 in sigma [xx,xy,yx,yy] \n",
    "    # Pi has shape of  (sample_size,num_mixtures)\n",
    "    # swap axis to have shape (num_mixtures,sample_size,parameter)\n",
    "    #print(\"Before anythinG: \",sigma_x,sigma_y, mu_x , mu_y,y)\n",
    "    result = bivirate_distribution(sigma_x,sigma_y, mu_x , mu_y,y) * pi\n",
    "    #print(\"result after: \",result)\n",
    "    result = torch.sum(result, dim=1)   \n",
    "    #print(\"result after sum: \",result)\n",
    "    result = -torch.log(result)\n",
    "    #print(\"result after log: \",result)\n",
    "    return torch.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bivirate_model(data_loader, model, optimizer,epochs=100):\n",
    "    ''' \n",
    "        sigma:  (num_samples,num_mixtures,2,2) \n",
    "        pi:     (num_samples,num_mixtures)\n",
    "        mue:    (num_samples,num_mixtures,2)\n",
    "\n",
    "        The last parameter '2' represents x and y  \n",
    "    '''\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i in range (epochs):\n",
    "        total_loss = 0\n",
    "        for X, y,in data_loader:\n",
    "            # print(X.shape)\n",
    "            #print(\"INput: \",X)\n",
    "            x_variable = Variable(X,requires_grad=True).to(device)\n",
    "            #X = X.to(device)\n",
    "            #y = y.to(device)\n",
    "            pi, sigma_x,sigma_y, mu_x , mu_y = model(x_variable)\n",
    "            #print(\"outputs: \",pi, sigma_x,sigma_y, mu_x , mu_y)\n",
    "            #pi_variable, sigma_variable, mu_variable = model(x_variable)\n",
    "            #print(f\"sigma_variable{sigma_variable.shape}\")\n",
    "            loss = mdn_loss_fn( pi, sigma_x,sigma_y, mu_x , mu_y, y)\n",
    "            # loss = Variable(loss, requires_grad = True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f\"Epoch {i+1} train loss: {avg_loss}\")\n",
    "    print(f\"Epoch {i+1} train loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_network = MDN_bivirate(n_hidden=30, n_gaussians=5).to(device)\n",
    "bi_optimizer = torch.optim.Adam(bi_network.parameters(), lr=0.001)\n",
    "# bi_network.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5099"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = torch.linspace(start=1, end=10, steps=10)\n",
    "# a = a.unsqueeze(dim=1)\n",
    "# a.expand(10,3)\n",
    "len(train_loader_one.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_bivirate_model(train_loader_one,bi_network,bi_optimizer)\n",
      "Cell \u001b[0;32mIn [41], line 15\u001b[0m, in \u001b[0;36mtrain_bivirate_model\u001b[0;34m(data_loader, model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (epochs):\n\u001b[1;32m     14\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mfor\u001b[39;00m X, y,_ \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     16\u001b[0m         \u001b[39m# print(X.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[39m#print(\"INput: \",X)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         x_variable \u001b[39m=\u001b[39m Variable(X,requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m         \u001b[39m#X = X.to(device)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[39m#y = y.to(device)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "train_bivirate_model(train_loader_one,bi_network,bi_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for X, y in train_loader_one:\n",
    "        pi_variable, sigma_x_variable,sigma_y_variable, mu_x_variable ,mu_y_variable = bi_network(X)\n",
    "        pi = pi_variable.detach().cpu().numpy()\n",
    "        sigma_x = sigma_x_variable.detach().cpu().numpy()\n",
    "        sigma_y = sigma_y_variable.detach().cpu().numpy()\n",
    "        mu_x = mu_x_variable.detach().cpu().numpy()\n",
    "        mu_y = mu_y_variable.detach().cpu().numpy()\n",
    "        value = (y.detach().cpu().numpy()).tolist()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(values):\n",
    "    return np.log(values/(1-values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(mus_x,mus_y,sigma_x,sigma_y,pi,samples=5):\n",
    "    mus = [np.array([mus_x[0], mus_y[0]]), np.array([mus_x[1], mus_x[2]]), np.array([mus_x[2], mus_y[2]]),np.array([mus_x[3], mus_y[3]]), np.array([mus_x[4], mus_y[4]])]\n",
    "    covs = [np.array([[sigma_x[0], 0], [0, sigma_y[0]]]), np.array([[sigma_x[1], 0], [0, sigma_y[1]]]), np.array([[sigma_x[2], 0], [0, sigma_y[2]]]),np.array([[sigma_x[3], 0], [0, sigma_y[3]]]),np.array([[sigma_x[4], 0], [0, sigma_y[4]]])]\n",
    "    pis = np.array(pi)\n",
    "    acc_pis = [np.sum(pis[:i]) for i in range(1, len(pis)+1)]\n",
    "    assert np.isclose(acc_pis[-1], 1)\n",
    "    # sample uniform\n",
    "    r = np.random.uniform(0, 1)\n",
    "    # select Gaussian\n",
    "    k = 0\n",
    "    for i, threshold in enumerate(acc_pis):\n",
    "        if r < threshold:\n",
    "            k = i\n",
    "            break\n",
    "    selected_mu = mus[k]\n",
    "    selected_cov = covs[k]\n",
    "    print(pis,k)\n",
    "\n",
    "    # sample from selected Gaussian\n",
    "    lambda_, gamma_ = np.linalg.eig(selected_cov)\n",
    "    dimensions = len(lambda_)\n",
    "    data=[]\n",
    "\n",
    "    for i in range(samples): \n",
    "        # sampling from normal distribution\n",
    "        y_s = np.random.uniform(0, 1, size=(dimensions*1, 3))\n",
    "        x_normal = np.mean(inv_sigmoid(y_s), axis=1).reshape((-1, dimensions))\n",
    "        # transforming into multivariate distribution\n",
    "        x_multi = (x_normal*lambda_) @ gamma_ + selected_mu\n",
    "\n",
    "        \n",
    "        y = x_multi.tolist()\n",
    "\n",
    "        data.append(y[0])\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_distribution(mu_x[5],mu_y[5],sigma_x[5],sigma_y[5],pi[5])\n",
    "data = np.array(data)\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plt.scatter(y[5][0],y[5][1],color= 'red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97275231ca3bc88d4d425f48fe4700083adb1bbc86d99b7fab9639fa13d0f98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
