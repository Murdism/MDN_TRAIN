{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt # creating visualizations\n",
    "import numpy as np # basic math and random numbers\n",
    "import torch # package for building functions with learnable parameters\n",
    "import torch.nn as nn # prebuilt functions specific to neural networks\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "#from disp import ADE,FDE,prediction_displacement,ADE_double_coordinates,FDE_double_coordinates,prediction_displacement_double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Predict error\n",
    "# Average Displacement error\n",
    "\n",
    "def ADE(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        for j in range (half):\n",
    "\n",
    "            a = np.array((pred[i][j] , pred[i][half+j]))\n",
    "            b = np.array((truth.iloc[i][j] , truth.iloc[i][half+j]))\n",
    "\n",
    "            dist = np.linalg.norm(a-b)\n",
    "            sum+=dist\n",
    "            counter+=1\n",
    "            #print(\"Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "\n",
    "    return (sum/counter)\n",
    "def prediction_displacement_double(pred): \n",
    "          \n",
    "        sum=0\n",
    "        counter=0        \n",
    "        pred=np.array(pred)\n",
    "        last_index= (len(pred[0])-1)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "                    a = np.array((pred[i][0][0] , pred[i][0][1]))\n",
    "                    b = np.array((pred[i][last_index][0] , pred[i][last_index][1]))\n",
    "\n",
    "                    dist = np.linalg.norm(a-b)\n",
    "                    sum+=dist\n",
    "                    counter+=1\n",
    "        return (sum/counter)\n",
    "\n",
    "def prediction_displacement(pred): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    pred=np.array(pred)\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        last=(len(pred[i]) - 1)\n",
    "\n",
    "        a = np.array((pred[i][0] , pred[i][half]))\n",
    "        b = np.array((pred[i][half-1] , pred[i][last]))\n",
    "\n",
    "        dist = np.linalg.norm(a-b)\n",
    "        sum+=dist\n",
    "        counter+=1\n",
    "        #print(\"FDE Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "            \n",
    "    return (sum/counter)\n",
    "def FDE(pred,truth): \n",
    "    counter=0\n",
    "    sum=0\n",
    "    for i in range(len(pred)):\n",
    "        half=int(len(pred[i])/2)\n",
    "        last=(len(pred[i]) - 1)\n",
    "\n",
    "        a = np.array((pred[i][half-1] , pred[i][last]))\n",
    "        b = np.array((truth.iloc[i][half-1] , truth.iloc[i][last]))\n",
    "\n",
    "        dist = np.linalg.norm(a-b)\n",
    "        sum+=dist\n",
    "        counter+=1\n",
    "        #print(\"FDE Distance between\",a,\" and \",b,\" is: \",dist)\n",
    "            \n",
    "    return (sum/counter)\n",
    "\n",
    "\n",
    "def FDE_double_coordinates(pred,truth):\n",
    "          \n",
    "        sum=0\n",
    "        counter=0\n",
    "        \n",
    "        pred=np.array(pred)\n",
    "        truth=np.array(truth)\n",
    "        last_index= (len(pred[0])-1)\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "            a = np.array((pred[i][last_index][0] , pred[i][last_index][1]))\n",
    "            b = np.array((truth[i][last_index][0] , truth[i][last_index][1]))\n",
    "\n",
    "            dist = np.linalg.norm(a-b)\n",
    "            sum+=dist\n",
    "            counter+=1\n",
    "\n",
    "                # for j in range (len(pred[i])):\n",
    "                #    pred_x.append(pred[i][j][0])  \n",
    "                #    pred_y.append(pred[i][j][1]) \n",
    "\n",
    "                #    truth_x.append(truth[i][j][0])  \n",
    "                #    truth_y.append(truth[i][j][1])\n",
    "        return (sum/counter)\n",
    "def ADE_double_coordinates(pred,truth):\n",
    "          \n",
    "        sum=0\n",
    "        counter=0\n",
    "        \n",
    "        pred=np.array(pred)\n",
    "        truth=np.array(truth)\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "                \n",
    "                for j in range (len(pred[i])):\n",
    "\n",
    "                    a = np.array((pred[i][j][0] , pred[i][j][1]))\n",
    "                    b = np.array((truth[i][j][0] , truth[i][j][1]))\n",
    "\n",
    "                    dist = np.linalg.norm(a-b)\n",
    "                    sum+=dist\n",
    "                    counter+=1\n",
    "        return (sum/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('train_input.pickle', 'rb') as data:\n",
    "     train_input = pickle.load(data)\n",
    "    with open('validate_input.pickle', 'rb') as data:\n",
    "         validate_input= pickle.load(data)\n",
    "    with open('test_input.pickle', 'rb') as data:\n",
    "         test_input = pickle.load(data)\n",
    "    with open('train_target.pickle', 'rb') as data:\n",
    "          train_target = pickle.load(data)\n",
    "    with open('validate_target.pickle', 'rb') as data:\n",
    "          validate_target = pickle.load(data)\n",
    "    with open('test_target.pickle', 'rb') as data:\n",
    "         test_target = pickle.load(data)\n",
    "    return torch.tensor(train_input,dtype=torch.float32), torch.tensor(validate_input,dtype=torch.float32), torch.tensor(test_input,dtype=torch.float32), torch.tensor(train_target,dtype=torch.float32), torch.tensor(validate_target,dtype=torch.float32), torch.tensor(test_target,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  torch.Size([5099, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "train_input,validate_input,test_input,train_target,validate_target,test_target=load_data()\n",
    "print(\"Train shape: \" ,train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNetwork, self).__init__()\n",
    "        self.output_shape = (8,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.unflatten = nn.Unflatten(1,self.output_shape)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(8*2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 8*2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # print(x[0])\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        out = self.unflatten(logits)\n",
    "        # print(out[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (unflatten): Unflatten(dim=1, unflattened_size=(8, 2))\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LinearNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input. requires_grad = True\n",
    "train_input. requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6634,  0.5903],\n",
      "        [ 0.4768, -0.8592],\n",
      "        [ 0.0575, -0.2041],\n",
      "        [ 0.4909, -1.4022],\n",
      "        [-0.4778,  1.3517],\n",
      "        [ 0.2441, -0.9381],\n",
      "        [-1.7007,  0.1728],\n",
      "        [-0.5561, -0.0388]], grad_fn=<SelectBackward0>)\n",
      "///////////\n",
      "tensor([-1.6634,  0.5903,  0.4768, -0.8592,  0.0575, -0.2041,  0.4909, -1.4022,\n",
      "        -0.4778,  1.3517,  0.2441, -0.9381, -1.7007,  0.1728, -0.5561, -0.0388],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([14.9352,  5.3071, 14.4947,  5.3293, 14.0542,  5.3515, 13.6137,  5.3737,\n",
      "        13.1726,  5.3937, 12.7264,  5.3937, 12.2802,  5.3937, 11.8340,  5.3937],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(train_input)\n",
    "y_pred.shape\n",
    "print (y_pred[0])\n",
    "sq = torch.flatten(y_pred,1)\n",
    "sq2 = torch.flatten(train_input,1)\n",
    "print(\"///////////\")\n",
    "print(sq[0])\n",
    "print(sq2[0])\n",
    "# temp = nn.Flatten(train_input)\n",
    "# train_input.shape,temp.shape,y_pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(47.2369)\n",
      "300 tensor(0.6566)\n",
      "600 tensor(0.7877)\n",
      "900 tensor(0.6171)\n",
      "1200 tensor(0.9639)\n",
      "1500 tensor(0.7391)\n",
      "1800 tensor(2.4894)\n",
      "2100 tensor(0.8386)\n",
      "2400 tensor(1.0475)\n",
      "2700 tensor(0.4876)\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for epoch in range(3000):\n",
    "        y_pred = model(train_input) # make a prediction\n",
    "        loss = loss_fn(y_pred,train_target) # compute the loss\n",
    "        optimizer.zero_grad() # prepare the optimizer\n",
    "        loss.backward() # compute the contribution of each parameter to the loss\n",
    "        optimizer.step() # modify the parameters\n",
    "\n",
    "        if epoch % 300 == 0:\n",
    "            print(epoch, loss.data)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2341)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_pred = model(test_input)\n",
    "    train_pred = model(train_input)\n",
    "    loss = loss_fn(test_target, test_pred)\n",
    "    print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_double(prediction_train,prediction_test,train_target,test_target):\n",
    "                # # Show error rate\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Train: \",prediction_displacement_double(train_target))\n",
    "        print(\"AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Test: \",prediction_displacement_double(test_target))\n",
    "             \n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TEST: \", ADE_double_coordinates(prediction_test,test_target))\n",
    "        #average_displacement_error\n",
    "        print(\"ADE ERROR RATE TRAIN: \", ADE_double_coordinates(prediction_train,train_target))\n",
    "        print(\"//////////////////////////////////////////\")\n",
    "        #Final_displacement_error\n",
    "        print(\"FDE ERROR RATE TEST: \", FDE_double_coordinates(prediction_test,test_target))\n",
    "        print(\"FDE ERROR RATE TRAIN: \", FDE_double_coordinates(prediction_train,train_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Train:  1.7491152588806866\n",
      "AVERAGE DISTANCE BETWEEN FIRST AND LAST POINT Test:  2.7941778349259665\n",
      "ADE ERROR RATE TEST:  1.424682301002654\n",
      "ADE ERROR RATE TRAIN:  1.5516014411372374\n",
      "//////////////////////////////////////////\n",
      "FDE ERROR RATE TEST:  1.419016819525893\n",
      "FDE ERROR RATE TRAIN:  1.5553257257172959\n"
     ]
    }
   ],
   "source": [
    "show_error_double(train_pred,test_pred,train_target,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_layers=12):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(1, self.hidden_layers)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, n_samples = [], y.size(0)\n",
    "        print('n_samples',n_samples)\n",
    "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            # input_t = time_step.s\n",
    "            print(\"time_step\",time_step.shape)\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            \n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = LSTM()\n",
    "criterion_LSTM = nn.MSELoss()\n",
    "optimiser_LSTM = torch.optim.LBFGS(model_LSTM.parameters(), lr=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(n_epochs, model, optimiser, loss_fn, \n",
    "                  train_input, train_target, test_input, test_target):\n",
    "    for i in range(n_epochs):\n",
    "        def closure():\n",
    "            optimiser.zero_grad()\n",
    "            out = model(train_input)\n",
    "            print(\"okkokokok\",out.shape)\n",
    "            loss = loss_fn(out, train_target)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimiser.step(closure)\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = model(test_input, future=future)\n",
    "            # use all pred samples, but only go to 999\n",
    "            loss = loss_fn(pred[:, :-future], test_target)\n",
    "            y = pred.detach().numpy()\n",
    "        # draw figures\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.title(f\"Step {i+1}\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        n = train_input.shape[1] # 999\n",
    "        def draw(yi, colour):\n",
    "            plt.plot(np.arange(n), yi[:n], colour, linewidth=2.0)\n",
    "            plt.plot(np.arange(n, n+future), yi[n:], colour+\":\", linewidth=2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'b')\n",
    "        draw(y[2], 'g')\n",
    "        plt.savefig(\"predict%d.png\"%i, dpi=200)\n",
    "        plt.close()\n",
    "        # print the loss\n",
    "        out = model(train_input)\n",
    "        loss_print = loss_fn(out, train_target)\n",
    "        print(\"Step: {}, Loss: {}\".format(i, loss_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_step torch.Size([5099, 1, 2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "LSTMCell: Expected input to be 1-D or 2-D but received 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_loop(n_epochs\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel_LSTM, optimiser\u001b[39m=\u001b[39;49moptimiser_LSTM, loss_fn\u001b[39m=\u001b[39;49mcriterion_LSTM, \n\u001b[1;32m      2\u001b[0m                   train_input\u001b[39m=\u001b[39;49mtrain_input, train_target\u001b[39m=\u001b[39;49mtrain_target, test_input\u001b[39m=\u001b[39;49mtest_input, test_target\u001b[39m=\u001b[39;49mtest_target)\n",
      "Cell \u001b[0;32mIn [107], line 11\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, model, optimiser, loss_fn, train_input, train_target, test_input, test_target)\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m---> 11\u001b[0m optimiser\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     future \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/optim/lbfgs.py:311\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    308\u001b[0m state\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mn_iter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[39m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m orig_loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    312\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    313\u001b[0m current_evals \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [107], line 6\u001b[0m, in \u001b[0;36mtraining_loop.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m():\n\u001b[1;32m      5\u001b[0m     optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m     out \u001b[39m=\u001b[39m model(train_input)\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mokkokokok\u001b[39m\u001b[39m\"\u001b[39m,out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(out, train_target)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [105], line 21\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, y, future_preds)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m time_step \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39msplit(\u001b[39m1\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# N, 1\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m# input_t = time_step.s\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtime_step\u001b[39m\u001b[39m\"\u001b[39m,time_step\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m     h_t, c_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm1(time_step, (h_t, c_t)) \u001b[39m# initial hidden and cell states\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     h_t2, c_t2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm2(h_t, (h_t2, c_t2)) \u001b[39m# new hidden and cell states\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(h_t2) \u001b[39m# output from the last FC layer\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torched/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1177\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hx: Optional[Tuple[Tensor, Tensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m-> 1177\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), \\\n\u001b[1;32m   1178\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTMCell: Expected input to be 1-D or 2-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1179\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n",
      "\u001b[0;31mAssertionError\u001b[0m: LSTMCell: Expected input to be 1-D or 2-D but received 3-D tensor"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=64, model=model_LSTM, optimiser=optimiser_LSTM, loss_fn=criterion_LSTM, \n",
    "                  train_input=train_input, train_target=train_target, test_input=test_input, test_target=test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 8]) torch.Size([2, 12]) torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.LSTMCell(8, 12) # (input_size, hidden_size)\n",
    "input = torch.randn(2, 3, 8) # (time_steps, batch, input_size)\n",
    "input = torch.randn(8, 2, 8) # (time_steps, batch, input_size)\n",
    "hx = torch.randn(2, 12) # (batch, hidden_size)\n",
    "cx = torch.randn(2, 12)\n",
    "output = []\n",
    "\n",
    "for i in range(input.size()[0]):\n",
    "    hx, cx = rnn(input[i], (hx, cx))\n",
    "    print(input[i].shape, hx.shape, cx.shape)\n",
    "    output.append(hx)\n",
    "output = torch.stack(output, dim=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torched')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b72875b808b4821a66c056bbb38de6a4c1e75d06274b19513bafe416aafb864f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
