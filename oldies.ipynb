{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt # creating visualizations\n",
    "import numpy as np # basic math and random numbers\n",
    "import torch # package for building functions with learnable parameters\n",
    "import torch.nn as nn # prebuilt functions specific to neural networks\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "#from disp import ADE,FDE,prediction_displacement,ADE_double_coordinates,FDE_double_coordinates,prediction_displacement_double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN_bivirate_old(nn.Module):\n",
    "    def __init__(self,n_hidden, n_gaussians,input_shape = 16):\n",
    "        super(MDN_bivirate_old, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(input_shape, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_sigma_x = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_sigma_y = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "        self.z_mu_x = nn.Linear(n_hidden, n_gaussians).to(device)  \n",
    "        self.z_mu_y = nn.Linear(n_hidden, n_gaussians).to(device)\n",
    "    \n",
    "    def diagonal(self,x):\n",
    "        \"\"\"Average first and last element of a 1-D array\"\"\"\n",
    "        return np.diag(np.full(2,[x[0],x[1]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape: \",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        z_h = self.z_h(x).to(device)\n",
    "        pi = nn.functional.softmax(self.z_pi(z_h), -1).to(device)\n",
    "        sigma_x = torch.exp(self.z_sigma_x(z_h)).to(device)\n",
    "        sigma_y = torch.exp(self.z_sigma_y(z_h)).to(device)\n",
    "        mu_x = self.z_mu_x(z_h).to(device)\n",
    "        mu_y = self.z_mu_y(z_h).to(device)\n",
    "        \n",
    "        # reshape to concatinate\n",
    "        mux_detached = mu_x.detach().cpu().numpy().reshape(mu_x.shape[0],mu_x.shape[1],1)\n",
    "        muy_detached = mu_y.detach().cpu().numpy().reshape(mu_y.shape[0],mu_x.shape[1],1)\n",
    "        sigmax_detached = sigma_x.detach().cpu().numpy().reshape((sigma_x.shape[0],sigma_x.shape[1],1))\n",
    "        sigmay_detached = sigma_x.detach().cpu().numpy().reshape((sigma_y.shape[0],sigma_y.shape[1],1))\n",
    "        # concatinate all mu and sigma values\n",
    "        # mu = torch.tensor(np.concatenate((mux_detached, muy_detached), axis=2)).to(device)\n",
    "        # sigma_xy = np.concatenate((sigmax_detached,sigmay_detached), axis=2)\n",
    "        mu =np.concatenate((mux_detached, muy_detached), axis=2)\n",
    "        sigma_xy = np.concatenate((sigmax_detached,sigmay_detached), axis=2)\n",
    "\n",
    "        # Change sigma to diagonal covariance matrix\n",
    "        axis = 2\n",
    "        #sigma = torch.tensor(np.apply_along_axis(self.diagonal,axis,sigma_xy)).to(device)\n",
    " \n",
    "        sigma = np.apply_along_axis(self.diagonal,axis,sigma_xy)\n",
    "        # print(\"sigma.shape: \",sigma.shape)\n",
    "        return pi, sigma, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn_old(pis, mus, sigmas, y):\n",
    "    results = []\n",
    "    # calculate the score for each mixture of the gaussian_distribution\n",
    "    # input shape (sample_size,num_mixtures,parameter) parametr is 2 in mue (x,y) and 2,2 in sigma [xx,xy,yx,yy] \n",
    "    # Pi has shape of  (sample_size,num_mixtures)\n",
    "    # swap axis to have shape (num_mixtures,sample_size,parameter)\n",
    "    mus = np.swapaxes(mus,0,1)\n",
    "    sigmas = np.swapaxes(sigmas,0,1)\n",
    "    pis = pis.detach().cpu().numpy()\n",
    "    pis = np.swapaxes(pis,0,1)\n",
    "    for mu,sigma,pi in zip(mus,sigmas,pis):\n",
    "        # print(f\"From for loop loss sigma: {sigma.shape}\")\n",
    "        result = multivariate_gaussian(y, mu, sigma) * pi\n",
    "        results.append(result)\n",
    "    result  = torch.tensor(results)\n",
    "    result = np.swapaxes(result,0,1)\n",
    "    # print(\"result1: \",result.shape)\n",
    "    result = torch.sum(result, dim=1)\n",
    "    # print(\"result after sum: \",result.shape)\n",
    "    result = -torch.log(result)\n",
    "    # print(\"result after log: \",result.shape)\n",
    "    return torch.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian(pos, mu, Sigma):\n",
    "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "    pos is an array constructed by packing the meshed arrays of variables\n",
    "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "    \"\"\"\n",
    "   # Bivariate\n",
    "    n = 2\n",
    "    dims = Sigma.ndim\n",
    "    if dims > 2:\n",
    "        results = []\n",
    "        for pos_i,sigma_i,mu_i in zip(pos,Sigma,mu):\n",
    "            Sigma_det = np.linalg.det(sigma_i)\n",
    "            Sigma_inv = np.linalg.inv(sigma_i)\n",
    "            N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "            pos_mu = pos_i- mu_i\n",
    "           #print(f\"mu.shape: {mu_i.shape}, Sigma: {sigma_i.shape} , N.shape: {N},pos.shape {pos_i.shape} pos_mu.shape {pos_mu.shape}\")\n",
    "            fac = np.einsum('...k,kl,...l->...', pos_mu, Sigma_inv, pos_mu)\n",
    "            result_i = np.exp(-fac / 2) / N\n",
    "            results.append(result_i)\n",
    "        \n",
    "        return np.array(results) \n",
    "\n",
    "    else:\n",
    "        Sigma_det = np.linalg.det(Sigma)\n",
    "        Sigma_inv = np.linalg.inv(Sigma)\n",
    "        N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "        # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "        # way across all the input variables.\n",
    "        # print(\"Multi-Dimentional\",pos.device)\n",
    "        pos_mu = pos.detach().cpu().numpy()-mu\n",
    "        #print(f\"mu.shape: {mu.shape}, Sigma: {Sigma.shape} , N.Sigma_det: {Sigma_det.shape},pos.shape {pos.shape} pos_mu.shape {pos_mu.shape}\")\n",
    "        fac = np.einsum('...k,kl,...l->...', pos_mu, Sigma_inv, pos_mu)\n",
    "\n",
    "        return np.exp(-fac / 2) / N\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5 (default, Dec  9 2021, 17:04:37) \n[GCC 8.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
